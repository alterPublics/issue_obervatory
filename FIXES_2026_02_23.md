# Error Message Improvements — 2026-02-23

## Summary

Fixed two issues reported from collection testing:
1. **GDELT empty error messages** — collection failures showed blank error messages
2. **RSS Feeds zero-record logging** — improved diagnostic logging when no matches are found

## Issue 1: GDELT Empty Error Messages

### Problem
When GDELT API requests failed due to network errors, the error message in the collection task UI was blank/empty. This happened because `str(exc)` could return an empty string for some `httpx` exception types.

### Root Cause
Three locations in `arenas/gdelt/collector.py` used this pattern:
```python
error_detail = f"{type(exc).__name__}: {str(exc)}" if str(exc) else type(exc).__name__
```

When `str(exc)` was empty, this became just `"ConnectError"` or similar, which provided minimal diagnostic value.

### Fix
Changed all three locations to use a fallback pattern that always provides context:

**Location 1: `_query_term()` method (lines 479-491)**
```python
exc_msg = str(exc)
error_detail = (
    f"{type(exc).__name__}: {exc_msg}"
    if exc_msg
    else f"{type(exc).__name__} (connection error)"
)
raise ArenaCollectionError(
    f"gdelt: request error for term='{term}' url='{GDELT_DOC_API_BASE}' — {error_detail}",
    arena="news_media",
    platform=self.platform_name,
) from exc
```

**Location 2: `health_check()` method (lines 377-383)**
```python
exc_msg = str(exc)
error_detail = (
    f"{type(exc).__name__}: {exc_msg}"
    if exc_msg
    else f"{type(exc).__name__} (network error)"
)
return {**base, "status": "down", "detail": f"Connection error: {error_detail}"}
```

**Location 3: JSON parsing in `_query_term()` (lines 533-544)**
```python
exc_msg = str(exc)
error_detail = (
    f"{type(exc).__name__}: {exc_msg}"
    if exc_msg
    else f"{type(exc).__name__} (JSON parsing failed)"
)
logger.warning(
    "gdelt: JSON parse error for term='%s' filter='%s' — %s — body: %s — skipping.",
    term,
    extra_filter,
    error_detail,
    body_snippet,
)
```

### Impact
Error messages now always contain meaningful context:
- Before: (empty string) or just "ConnectError"
- After: "ConnectError (connection error)" or "ConnectError: Network is unreachable"

## Issue 2: RSS Feeds Zero-Record Logging

### Problem
RSS feeds collection completed with 0 records for "grønland" search term. The existing logging didn't clearly distinguish between:
1. Feeds failing to fetch (HTTP errors)
2. Feeds returning 0 items
3. Feeds returning items but none matching search terms

This made it hard to diagnose whether the problem was network connectivity, empty feeds, or simply that no recent articles mentioned the search term (which is expected for RSS feeds — they only contain recent items).

### Root Cause
RSS feeds are FORWARD_ONLY (temporal mode), so they only capture recent content. If no feed has published an article about "grønland" in the last few hours/days, 0 results is the correct and expected outcome. The logging needed to make this clearer.

### Fixes

**Fix 1: Distinguish zero-fetch from zero-matches (lines 202-213)**
```python
async with self._build_http_client() as client:
    raw_entries = await self._fetch_feeds(client, effective_feeds)

logger.info(
    "rss_feeds: collect_by_terms — fetched %d total entries from %d feeds",
    len(raw_entries),
    len(effective_feeds),
)

# If no entries fetched at all, log a warning
if not raw_entries:
    logger.warning(
        "rss_feeds: collect_by_terms — NO ENTRIES FETCHED from %d feeds. "
        "All feeds may be empty or failed to fetch. Check feed URLs and network connectivity.",
        len(effective_feeds),
    )
```

**Fix 2: Improved zero-match diagnostic logging (lines 252-278)**
```python
# If no matches found, log diagnostic information
if not all_records and raw_entries:
    logger.warning(
        "rss_feeds: NO MATCHES FOUND for search terms %s. "
        "%d entries were fetched but none matched the search criteria. "
        "This is expected for RSS feeds which only contain recent items. "
        "Sample entry titles for debugging:",
        lower_terms[:5],  # Log first 5 search terms
        len(raw_entries),
    )
    for i, (feed_key, _, entry) in enumerate(raw_entries[:10]):
        title = getattr(entry, "title", "NO TITLE")
        summary = getattr(entry, "summary", "") or getattr(entry, "description", "")
        summary_preview = _strip_html(summary)[:100] if summary else "NO SUMMARY"
        logger.warning(
            "  [%d] %s\n      Title: %s\n      Summary: %s...",
            i + 1,
            feed_key,
            title,
            summary_preview,
        )
```

**Fix 3: Include response body in HTTP error logging (lines 587-593)**
```python
if response.status_code >= 400:
    # Include response body snippet for diagnostic value
    body_snippet = response.text[:200] if response.text else "(empty body)"
    logger.warning(
        "rss_feeds: '%s' returned HTTP %d — %s — skipping.",
        feed_key,
        response.status_code,
        body_snippet,
    )
    return []
```

**Fix 4: Log empty feeds explicitly (lines 618-622)**
```python
# Log if feed parsed successfully but has no entries
if not feed.entries:
    logger.debug(
        "rss_feeds: feed '%s' parsed successfully but contains 0 entries (feed may be empty)",
        feed_key,
    )
```

### Impact
Logs now clearly distinguish:
1. **Network/fetch failures**: "NO ENTRIES FETCHED from N feeds"
2. **Empty feeds**: "parsed successfully but contains 0 entries"
3. **No matches**: "NO MATCHES FOUND for search terms [...]... This is expected for RSS feeds..."
4. **HTTP errors**: Include response body snippet for debugging

This makes it immediately clear when 0 results is expected behavior vs when there's an actual problem.

## Testing

### Danish Character Matching (Verified)
The RSS collector uses `query_builder.match_groups_in_text()` which implements word-boundary regex matching with Danish compound word support (lines 304-328 in `query_builder.py`):
- Terms > 2 chars use left boundary only: `\b{term}` (allows "grønland" to match "Grønlandspolitik")
- Short terms ≤ 2 chars use strict boundaries: `\b{term}\b` (prevents "i" matching inside "politik")

This is working correctly. The zero-results for "grønland" is expected because Danish RSS feeds simply haven't published articles about Greenland in the recent period covered by the feeds.

### Files Modified
1. `/src/issue_observatory/arenas/gdelt/collector.py` — 3 error handling improvements
2. `/src/issue_observatory/arenas/rss_feeds/collector.py` — 4 logging improvements

### Files Not Modified
- `query_builder.py` — Danish character matching already working correctly
- `tasks.py` — Error messages are propagated from collector, no changes needed

## Deployment Notes

These are logging and error message improvements only. No schema changes, no API changes, no breaking changes. Safe to deploy immediately.

Users will now see:
- Meaningful error messages when GDELT collection fails
- Clear explanations in logs when RSS feeds return 0 results
- Better diagnostic information for troubleshooting

## Follow-up Work (Optional)

1. Consider adding a UI hint on RSS feeds arena that it's FORWARD_ONLY (only captures recent content)
2. Consider adding expected date range to RSS collection results (e.g., "Checked feeds from last 24 hours")
3. Consider adding a "last successful fetch" timestamp to the feed cache for monitoring feed staleness
